{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from ray import tune\n",
    "import os\n",
    "import datetime\n",
    "import uuid\n",
    "import time\n",
    "import logging\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'Try_3' does not exist. Creating a new experiment\n",
      "Experiment Name:  Try_3\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = 'Try_3'\n",
    "#mlflow.set_tracking_uri(URI) # If you have an instance\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "print(\"Experiment Name: \", EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "DATA_PATH = '/Users/camilovelasquez/Desktop/Documents/Datasets/WISDM-Smartphones/wisdm-dataset/raw'\n",
    "ids = np.arange(1600, 1650)\n",
    "devices = ['phone']\n",
    "sensors = ['accel']\n",
    "activities = ['A', 'B']\n",
    "time_taken = 3000\n",
    "time_split = 100\n",
    "\n",
    "train_batch_size = 8\n",
    "eval_batch_size = 16\n",
    "epochs = 5\n",
    "\n",
    "train_size = 2500\n",
    "valid_size = 220\n",
    "test_size = 220\n",
    "\n",
    "\"\"\" \n",
    "Data : \n",
    "    Source of Dataset, Features Added, Structure, etc\n",
    "\n",
    "Splitting (Adequating tf.datasets) :\n",
    "        Train size, test size, batch_size, epochs, etc\n",
    "\n",
    "Preprocessing :\n",
    "        Sampling, Normalization, Scalers, Imputers, etc\n",
    "        \n",
    "Model : \n",
    "    Model architecture, characteristics, attributes\n",
    "Loss : \n",
    "    Loss functions, custom losses.\n",
    "Optimizers : \n",
    "    Optimizers and its parameters\n",
    "Metrics : \n",
    "    Metrics and custom metrics\n",
    "train : \n",
    "    Class weights, callbacks, steps per epoch\n",
    "\"\"\"\n",
    "                \n",
    "params = {'data': {'value' : DATA_PATH, \n",
    "                   'params' : {'ids' : ids, \n",
    "                               'devices' : devices, \n",
    "                               'sensors' : sensors, \n",
    "                               'activities' : activities, \n",
    "                               'time_taken' : time_taken, \n",
    "                               'time_split' : time_split}},\n",
    "          'split': {'value' : None, # IF you have a function with X splliting ways\n",
    "                    'params' : {'train_size' : train_size, \n",
    "                                'valid_size' : valid_size, \n",
    "                                'test_size' : test_size, \n",
    "                                'train_batch_size' : train_batch_size, \n",
    "                                'eval_batch_size' : eval_batch_size,\n",
    "                                'epochs' : epochs}},\n",
    "          'preprocess' : {'value' : None, \n",
    "                          'params' : {}},\n",
    "          'model' : {'value' : 'base_conv', \n",
    "                     'params' : {'layers' : 3,\n",
    "                                 'filters_0' : 32, \n",
    "                                 'filters_1' : 32, \n",
    "                                 'kernel_0': 7, \n",
    "                                 'kernel_1' : 9}},\n",
    "          'loss' : {'value' : 'binary_crossentropy', \n",
    "                    'params' : {}},\n",
    "          'optimizer' : {'value' : 'adam',\n",
    "                         'params' : {'lr' : 0.001}}, \n",
    "          'metrics' : {'value' : 'accuracy', \n",
    "                       'params': {}}, \n",
    "          'train' : {'value' : None, \n",
    "                   'params' : {'class_weight' : None, \n",
    "                               'callbacks' : ['es', 'cp'], \n",
    "                               'steps_per_epoch' : epochs//train_batch_size}}\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "def read_WISDM_data(DATA_PATH, ids=np.arange(1600, 1650), \n",
    "                    devices=['phone'], sensors=['accel']):\n",
    "    \"\"\"Read from DATA PATH and create a pandas table from it\"\"\"\n",
    "    i = 0\n",
    "    for current_id in ids:\n",
    "        for current_device in devices:\n",
    "            for current_sensor in sensors:\n",
    "                file_path = os.path.join(DATA_PATH, current_device, current_sensor, \n",
    "                                         'data_{}_{}_{}.txt'.format(current_id, current_sensor, current_device))\n",
    "                if i==0:\n",
    "                    table = pd.read_csv(file_path, delimiter=',', \n",
    "                                        names=['ID', 'Activity Label', 'Timestamp', 'x', 'y', 'z'], \n",
    "                                        lineterminator='\\n')\n",
    "                else:\n",
    "                    aux = pd.read_csv(file_path, delimiter=',', \n",
    "                                      names=['ID', 'Activity Label', 'Timestamp', 'x', 'y', 'z'], \n",
    "                                        lineterminator='\\n')\n",
    "                    table = pd.concat([table, aux], axis=0)\n",
    "                i+=1\n",
    "    table.loc[:,'z'] = table.z.str.replace(';','').astype(np.float32)\n",
    "    return table\n",
    "\n",
    "def transform_data(table, time_taken, time_split):\n",
    "    \"\"\"Transform data from raw table into a zip of (features, labels),\n",
    "        where features has shape (samples, time_steps, features), and labels (samples,)\"\"\"\n",
    "    table = table.set_index(['ID', 'Activity Label'])\n",
    "    table = table.groupby(['ID', 'Activity Label']).head(time_taken)\n",
    "    timestamp_edit = np.tile(np.arange(0,time_split), int(table.shape[0]/time_split))\n",
    "    table['Timestamp'] = timestamp_edit\n",
    "    table = table.reset_index().set_index(['ID', 'Activity Label', 'Timestamp'], append=True)\n",
    "    features = table.values.reshape((int(table.shape[0]/time_split), time_split, table.shape[1]))\n",
    "    labels = table.reset_index()['Activity Label']\\\n",
    "        .values[np.arange(0,int(table.shape[0]/time_split)*time_split, time_split)]\n",
    "    return features, labels\n",
    "\n",
    "def preprocessing_data(table, time_taken=3000, time_split=100, activities=['A', 'B']):\n",
    "    \"\"\"Preprocess table and convert it into tf dataset\"\"\"\n",
    "    features, labels = transform_data(table, time_taken=time_taken, time_split=time_split)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({'feature': features}, {'label': labels}))\n",
    "    ds = dataset.filter(lambda x, y: tf.reduce_any(tf.equal(y['label'], activities))==True)\n",
    "    ds = ds.map(label2prob)\n",
    "    return ds\n",
    "\n",
    "def label2prob(feature, label):\n",
    "    new_label = tf.where(tf.equal(label['label'], 'A'), 1, 0)\n",
    "    label['label'] = new_label\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model params\n",
    "def build_model(time_split):\n",
    "    inputs = tf.keras.Input(shape=(time_split, 3), name='feature')\n",
    "    x = tf.keras.layers.BatchNormalization(axis=2)(inputs)\n",
    "    x_1 = tf.keras.layers.Conv1D(filters=32, kernel_size=15)(x)\n",
    "    x_1 = tf.keras.layers.GlobalMaxPool1D()(x_1)\n",
    "    x_3 = tf.keras.layers.Conv1D(filters=32, kernel_size=31)(x)\n",
    "    x_3 = tf.keras.layers.GlobalMaxPool1D()(x_3)\n",
    "    x_5 = tf.keras.layers.Conv1D(filters=32, kernel_size=63)(x)\n",
    "    x_5 = tf.keras.layers.GlobalMaxPool1D()(x_5)\n",
    "    x = tf.keras.layers.Concatenate()([x_1, x_3, x_5])\n",
    "    x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid', name='label')(x)\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=output)\n",
    "    return model\n",
    "\n",
    "def build_optimizer():\n",
    "    optimizer='adam'\n",
    "    return optimizer\n",
    "\n",
    "def build_loss():\n",
    "    loss='binary_crossentropy'\n",
    "    return loss\n",
    "\n",
    "def build_metrics():\n",
    "    metrics=['accuracy']\n",
    "    return metrics\n",
    "\n",
    "def compile_model(model, optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']):\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_ds=None, valid_ds=None):\n",
    "    model.fit()\n",
    "\n",
    "def myprint(s, filename='modelsummary.txt'):\n",
    "    with open(filename,'w+') as f:\n",
    "        print(s, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_callbacks(params):\n",
    "    callbacks = []\n",
    "    if 'es' in params['train']['params']['callbacks']:\n",
    "        es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                               patience=params['split']['params']['epochs']//2,\n",
    "                                               mode='max',\n",
    "                                               restore_best_weights=True)\n",
    "        callbacks.append(es_callback)\n",
    "    elif 'cp' in params['train']['params']['callbacks']:\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         verbose=1)\n",
    "        callbacks.append(cp_callback)\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_key_value_pair(kv_pairs, key, dictionary):\n",
    "    if type(dictionary) is dict:\n",
    "        for new_key, new_value in dictionary.items():\n",
    "            if key is not None:\n",
    "                transform_key_value_pair(kv_pairs, key + '_' + str(new_key), new_value)\n",
    "            else:\n",
    "                transform_key_value_pair(kv_pairs, new_key, new_value)    \n",
    "    else:\n",
    "        kv_pairs[key] = dictionary\n",
    "        \n",
    "def example(**args):\n",
    "    print(args)\n",
    "    for arg in args:\n",
    "        print(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 312 steps\n",
      "Epoch 1/10\n",
      "312/312 [==============================] - 5s 15ms/step - loss: 0.5133 - accuracy: 0.7560 - val_loss: 0.2964 - val_accuracy: 0.8455\n",
      "Epoch 2/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 3s 8ms/step - loss: 0.2334 - accuracy: 0.9319 - val_loss: 0.1642 - val_accuracy: 0.9545\n",
      "Epoch 3/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 3s 9ms/step - loss: 0.0417 - accuracy: 0.9892 - val_loss: 0.1060 - val_accuracy: 0.9773\n",
      "Epoch 4/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 3s 11ms/step - loss: 0.0199 - accuracy: 0.9960 - val_loss: 0.0706 - val_accuracy: 0.9864\n",
      "Epoch 5/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 2s 8ms/step - loss: 0.0164 - accuracy: 0.9980 - val_loss: 0.1279 - val_accuracy: 0.9727\n",
      "Epoch 6/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 3s 9ms/step - loss: 0.0058 - accuracy: 0.9992 - val_loss: 0.0847 - val_accuracy: 0.9818\n",
      "Epoch 7/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 3s 8ms/step - loss: 0.0043 - accuracy: 0.9996 - val_loss: 0.0803 - val_accuracy: 0.9864\n",
      "Epoch 8/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 2s 8ms/step - loss: 0.0062 - accuracy: 0.9996 - val_loss: 0.0865 - val_accuracy: 0.9818\n",
      "Epoch 9/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 3s 9ms/step - loss: 0.0028 - accuracy: 0.9988 - val_loss: 0.7559 - val_accuracy: 0.7955\n",
      "Epoch 10/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 3s 9ms/step - loss: 0.0592 - accuracy: 0.9984 - val_loss: 0.1116 - val_accuracy: 0.9773\n",
      "14/14 [==============================] - 1s 85ms/step - loss: 0.1655 - accuracy: 0.9773\n"
     ]
    }
   ],
   "source": [
    "date = datetime.datetime.now().strftime('%Y_%m_%d')\n",
    "summary_filename = 'summary.txt'\n",
    "run_name = date + str(uuid.uuid4())[:5]\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    start_time = time.time()\n",
    "    # Params\n",
    "    params_kvpairs = dict()\n",
    "    transform_key_value_pair(params_kvpairs, None, params)\n",
    "    for k, v in params_kvpairs.items():\n",
    "        if type(v) in [list, dict, str, int, float, bool, tuple]:\n",
    "            mlflow.log_param(k, v)\n",
    "    complete_param = time.time()\n",
    "    \n",
    "    # Preprocess\n",
    "    table = read_WISDM_data(DATA_PATH, ids=ids, devices=devices, sensors=sensors)\n",
    "    ds = preprocessing_data(table, time_taken=time_taken, time_split=time_split, activities=activities)\n",
    "    complete_preprocess = time.time()\n",
    "    \n",
    "    #Model\n",
    "    model = build_model(time_split=time_split)\n",
    "    optimizer = build_optimizer()\n",
    "    loss = build_loss()\n",
    "    metrics = build_metrics()\n",
    "    model = compile_model(model=model, optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    model.summary(print_fn=lambda x: myprint(x, summary_filename))\n",
    "    \n",
    "    #Build dataset\n",
    "    ds_train = ds.take(train_size)\n",
    "    ds_train = ds_train.shuffle(buffer_size=TRAIN_BATCH_SIZE)\n",
    "    ds_train = ds_train.repeat(count=EPOCHS)\n",
    "    ds_train = ds_train.batch(TRAIN_BATCH_SIZE)\n",
    "    ds_train = ds_train.prefetch(1)\n",
    "\n",
    "    ds_valid = ds.skip(train_size).take(valid_size)\n",
    "    ds_valid = ds_valid.repeat(count=1)\n",
    "    ds_valid = ds_valid.batch(TRAIN_BATCH_SIZE)\n",
    "    ds_valid = ds_valid.prefetch(1)\n",
    "\n",
    "    ds_test = ds.skip(train_size+valid_size)\n",
    "    ds_test = ds_test.batch(EVAL_BATCH_SIZE)\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(ds_train, \n",
    "                    validation_data=ds_valid,\n",
    "                    steps_per_epoch=train_size//TRAIN_BATCH_SIZE,  \n",
    "                    epochs=EPOCHS, \n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1, \n",
    "                    shuffle=False)\n",
    "    complete_train = time.time()\n",
    "    model.save('model_'+ run_name + '.h5')\n",
    "    \n",
    "    #Evaluate\n",
    "    test_result = model.evaluate(ds_test)\n",
    "    complete_evaluation = time.time()\n",
    "    \n",
    "    # Times\n",
    "    preprocess_time = complete_preprocess - start_time\n",
    "    train_time = complete_train - complete_preprocess\n",
    "    evaluate_time = complete_evaluation - complete_train\n",
    "    \n",
    "    #Metrics\n",
    "    mlflow.log_metric('val_accuracy', max(history.history['val_accuracy']))\n",
    "    \n",
    "    #Artifacts\n",
    "    mlflow.log_artifact(summary_filename)\n",
    "    \n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using params for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "DATA_PATH = '/Users/camilovelasquez/Desktop/Documents/Datasets/WISDM-Smartphones/wisdm-dataset/raw'\n",
    "ids = np.arange(1600, 1650)\n",
    "devices = ['phone']\n",
    "sensors = ['accel']\n",
    "activities = ['A', 'B']\n",
    "time_taken = 3000\n",
    "time_split = 100\n",
    "\n",
    "train_batch_size = 8\n",
    "eval_batch_size = 16\n",
    "epochs = 5\n",
    "\n",
    "train_size = 2500\n",
    "valid_size = 220\n",
    "test_size = 220\n",
    "\"\"\" \n",
    "Data : \n",
    "    Source of Dataset, Features Added, Structure, etc\n",
    "\n",
    "Splitting (Adequating tf.datasets) :\n",
    "        Train size, test size, batch_size, epochs, etc\n",
    "\n",
    "Preprocessing :\n",
    "        Sampling, Normalization, Scalers, Imputers, etc\n",
    "        \n",
    "Model : \n",
    "    Model architecture, characteristics, attributes\n",
    "Loss : \n",
    "    Loss functions, custom losses.\n",
    "Optimizers : \n",
    "    Optimizers and its parameters\n",
    "Metrics : \n",
    "    Metrics and custom metrics\n",
    "train : \n",
    "    Class weights, callbacks, steps per epoch\n",
    "callbacks : \n",
    "    Callbacks and its parameters\n",
    "\"\"\"\n",
    "params = {'data': {'value' : DATA_PATH, \n",
    "                   'params' : {'ids' : ids, \n",
    "                               'devices' : devices, \n",
    "                               'sensors' : sensors, \n",
    "                               'activities' : activities, \n",
    "                               'time_taken' : time_taken, \n",
    "                               'time_split' : time_split}},\n",
    "          'split': {'value' : None, # IF you have a function with X splliting ways\n",
    "                    'params' : {'train_size' : train_size, \n",
    "                                'valid_size' : valid_size, \n",
    "                                'test_size' : test_size, \n",
    "                                'train_batch_size' : train_batch_size, \n",
    "                                'eval_batch_size' : eval_batch_size,\n",
    "                                'epochs' : epochs}},\n",
    "          'preprocess' : {'value' : None, \n",
    "                          'params' : {}},\n",
    "          'model' : {'value' : 'base_conv', \n",
    "                     'params' : {'layers' : 3,\n",
    "                                 'filters_0' : 32, \n",
    "                                 'filters_1' : 32, \n",
    "                                 'kernel_0': 7, \n",
    "                                 'kernel_1' : 9}},\n",
    "          'loss' : {'value' : 'binary_crossentropy', \n",
    "                    'params' : {}},\n",
    "          'optimizer' : {'value' : 'adam',\n",
    "                         'params' : {'lr' : 0.001}}, \n",
    "          'metrics' : {'value' : 'accuracy', \n",
    "                       'params': {}}, \n",
    "          'train' : {'value' : None, \n",
    "                   'params' : {'class_weight' : None, \n",
    "                               'steps_per_epoch' : epochs//train_batch_size}},\n",
    "          'callbacks' : {'es' : {}, # Early Stopping \n",
    "                         'tb' : {}, # TensorBoard\n",
    "                         'cp' : {}, # Checkpoint \n",
    "                         'ls' : {}, # Learning Rate Scheduler\n",
    "                        }\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "def read_WISDM_data(data_path, ids=np.arange(1600, 1650), \n",
    "                    devices=['phone'], sensors=['accel'], **kwargs):\n",
    "    \"\"\"Read from DATA PATH and create a pandas table from it\n",
    "    Read from params['data']\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    for current_id in ids:\n",
    "        for current_device in devices:\n",
    "            for current_sensor in sensors:\n",
    "                file_path = os.path.join(data_path, current_device, current_sensor, \n",
    "                                         'data_{}_{}_{}.txt'.format(current_id, current_sensor, current_device))\n",
    "                if i==0:\n",
    "                    table = pd.read_csv(file_path, delimiter=',', \n",
    "                                        names=['ID', 'Activity Label', 'Timestamp', 'x', 'y', 'z'], \n",
    "                                        lineterminator='\\n')\n",
    "                else:\n",
    "                    aux = pd.read_csv(file_path, delimiter=',', \n",
    "                                      names=['ID', 'Activity Label', 'Timestamp', 'x', 'y', 'z'], \n",
    "                                        lineterminator='\\n')\n",
    "                    table = pd.concat([table, aux], axis=0)\n",
    "                i+=1\n",
    "    table.loc[:,'z'] = table.z.str.replace(';','').astype(np.float32)\n",
    "    return table\n",
    "\n",
    "def transform_data(table, time_taken, time_split):\n",
    "    \"\"\"Transform data from raw table into a zip of (features, labels),\n",
    "        where features has shape (samples, time_steps, features), and labels (samples,)\"\"\"\n",
    "    table = table.set_index(['ID', 'Activity Label'])\n",
    "    table = table.groupby(['ID', 'Activity Label']).head(time_taken)\n",
    "    timestamp_edit = np.tile(np.arange(0,time_split), int(table.shape[0]/time_split))\n",
    "    table['Timestamp'] = timestamp_edit\n",
    "    table = table.reset_index().set_index(['ID', 'Activity Label', 'Timestamp'], append=True)\n",
    "    features = table.values.reshape((int(table.shape[0]/time_split), time_split, table.shape[1]))\n",
    "    labels = table.reset_index()['Activity Label']\\\n",
    "        .values[np.arange(0,int(table.shape[0]/time_split)*time_split, time_split)]\n",
    "    return features, labels\n",
    "\n",
    "def preprocessing_data(table, time_taken=3000, time_split=100, activities=['A', 'B'], **kwargs):\n",
    "    \"\"\"Preprocess table and convert it into tf dataset\"\"\"\n",
    "    features, labels = transform_data(table, time_taken=time_taken, time_split=time_split)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({'feature': features}, {'label': labels}))\n",
    "    ds = dataset.filter(lambda x, y: tf.reduce_any(tf.equal(y['label'], activities))==True)\n",
    "    ds = ds.map(label2prob)\n",
    "    return ds\n",
    "\n",
    "def label2prob(feature, label):\n",
    "    new_label = tf.where(tf.equal(label['label'], 'A'), 1, 0)\n",
    "    label['label'] = new_label\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model params\n",
    "def build_model(time_split, **kwargs):\n",
    "    inputs = tf.keras.Input(shape=(time_split, 3), name='feature')\n",
    "    x = tf.keras.layers.BatchNormalization(axis=2)(inputs)\n",
    "    x_1 = tf.keras.layers.Conv1D(filters=32, kernel_size=15)(x)\n",
    "    x_1 = tf.keras.layers.GlobalMaxPool1D()(x_1)\n",
    "    x_3 = tf.keras.layers.Conv1D(filters=32, kernel_size=31)(x)\n",
    "    x_3 = tf.keras.layers.GlobalMaxPool1D()(x_3)\n",
    "    x_5 = tf.keras.layers.Conv1D(filters=32, kernel_size=63)(x)\n",
    "    x_5 = tf.keras.layers.GlobalMaxPool1D()(x_5)\n",
    "    x = tf.keras.layers.Concatenate()([x_1, x_3, x_5])\n",
    "    x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid', name='label')(x)\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=output)\n",
    "    return model\n",
    "\n",
    "def build_optimizer():\n",
    "    optimizer='adam'\n",
    "    return optimizer\n",
    "\n",
    "def build_loss():\n",
    "    loss='binary_crossentropy'\n",
    "    return loss\n",
    "\n",
    "def build_metrics():\n",
    "    metrics=['accuracy']\n",
    "    return metrics\n",
    "\n",
    "def compile_model(model, optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']):\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_ds=None, valid_ds=None):\n",
    "    model.fit()\n",
    "\n",
    "def myprint(s, filename='modelsummary.txt'):\n",
    "    with open(filename,'w+') as f:\n",
    "        print(s, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = read_WISDM_data(params['data']['value'], **params['data']['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = preprocessing_data(table, **params['data']['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    mlflow.log_param(k,v)\n",
    "    mlflow.log_metric(metric, [1,2,3])\n",
    "    mlflow.set_tag(k, v)\n",
    "    mlflow.log_artifact('algo.json')\n",
    "    mlflow.log_artifact('algo.png')\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "pip install mlflow\n",
    "\n",
    "\n",
    "Directory where mlruns folder is located\n",
    ">> mlflow ui\n",
    "\"\"\"\n",
    "\n",
    "def transform_key_value_pair(kv_pairs, key, dictionary):\n",
    "    \"\"\"Flatten dicts in sum_keys: value single dict\"\"\"\n",
    "    if type(dictionary) is dict:\n",
    "        for new_key, new_value in dictionary.items():\n",
    "            if key is not None:\n",
    "                transform_key_value_pair(kv_pairs, key + '_' + str(new_key), new_value)\n",
    "            else:\n",
    "                transform_key_value_pair(kv_pairs, new_key, new_value)    \n",
    "    else:\n",
    "        kv_pairs[key] = dictionary\n",
    "\n",
    "        \n",
    "params_kvpairs = dict()\n",
    "transform_key_value_pair(params_kvpairs, None, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlflow_tune]",
   "language": "python",
   "name": "conda-env-mlflow_tune-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
