{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "import os\n",
    "import datetime\n",
    "import uuid\n",
    "import time\n",
    "import logging\n",
    "from hyperopt import (fmin, tpe, hp, STATUS_OK, STATUS_FAIL, Trials)\n",
    "idx = pd.IndexSlice\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "DATA_PATH = '/Users/camilovelasquez/Desktop/Documents/Datasets/WISDM-Smartphones/wisdm-dataset/raw'\n",
    "ids = np.arange(1600, 1650)\n",
    "devices = ['phone']\n",
    "sensors = ['accel']\n",
    "activities = ['A', 'B']\n",
    "time_taken = 3000\n",
    "time_split = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "eval_batch_size = 16\n",
    "epochs = 5\n",
    "\n",
    "train_size = 2500\n",
    "valid_size = 220\n",
    "test_size = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "def read_WISDM_data(DATA_PATH, ids=np.arange(1600, 1650), \n",
    "                    devices=['phone'], sensors=['accel']):\n",
    "    \"\"\"Read from DATA PATH and create a pandas table from it\"\"\"\n",
    "    i = 0\n",
    "    for current_id in ids:\n",
    "        for current_device in devices:\n",
    "            for current_sensor in sensors:\n",
    "                file_path = os.path.join(DATA_PATH, current_device, current_sensor, \n",
    "                                         'data_{}_{}_{}.txt'.format(current_id, current_sensor, current_device))\n",
    "                if i==0:\n",
    "                    table = pd.read_csv(file_path, delimiter=',', \n",
    "                                        names=['ID', 'Activity Label', 'Timestamp', 'x', 'y', 'z'], \n",
    "                                        lineterminator='\\n')\n",
    "                else:\n",
    "                    aux = pd.read_csv(file_path, delimiter=',', \n",
    "                                      names=['ID', 'Activity Label', 'Timestamp', 'x', 'y', 'z'], \n",
    "                                        lineterminator='\\n')\n",
    "                    table = pd.concat([table, aux], axis=0)\n",
    "                i+=1\n",
    "    table.loc[:,'z'] = table.z.str.replace(';','').astype(np.float32)\n",
    "    return table\n",
    "\n",
    "def transform_data(table, time_taken, time_split):\n",
    "    \"\"\"Transform data from raw table into a zip of (features, labels),\n",
    "        where features has shape (samples, time_steps, features), and labels (samples,)\"\"\"\n",
    "    table = table.set_index(['ID', 'Activity Label'])\n",
    "    table = table.groupby(['ID', 'Activity Label']).head(time_taken)\n",
    "    timestamp_edit = np.tile(np.arange(0,time_split), int(table.shape[0]/time_split))\n",
    "    table['Timestamp'] = timestamp_edit\n",
    "    table = table.reset_index().set_index(['ID', 'Activity Label', 'Timestamp'], append=True)\n",
    "    features = table.values.reshape((int(table.shape[0]/time_split), time_split, table.shape[1]))\n",
    "    labels = table.reset_index()['Activity Label']\\\n",
    "        .values[np.arange(0,int(table.shape[0]/time_split)*time_split, time_split)]\n",
    "    return features, labels\n",
    "\n",
    "def preprocessing_data(table, time_taken=3000, time_split=100, activities=['A', 'B']):\n",
    "    \"\"\"Preprocess table and convert it into tf dataset\"\"\"\n",
    "    features, labels = transform_data(table, time_taken=time_taken, time_split=time_split)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({'feature': features}, {'label': labels}))\n",
    "    ds = dataset.filter(lambda x, y: tf.reduce_any(tf.equal(y['label'], activities))==True)\n",
    "    ds = ds.map(label2prob)\n",
    "    return ds\n",
    "\n",
    "def label2prob(feature, label):\n",
    "    new_label = tf.where(tf.equal(label['label'], 'A'), 1, 0)\n",
    "    label['label'] = new_label\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = read_WISDM_data(DATA_PATH, ids=ids, devices=devices, sensors=sensors)\n",
    "ds = preprocessing_data(table, time_taken=time_taken, time_split=time_split, activities=activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'sampling': hp.choice('sampling', [\n",
    "                                {'type' : 'under', \n",
    "                                 'low_perc' : hp.choice('low_perc', [0.1, 0.2, 0.3])},\n",
    "                                {'type' : 'over', \n",
    "                                 'low_perc' : hp.choice('low_perc', [0.1, 0.2, 0.3])}]),\n",
    "          'batch_size' : hp.choice('batch_size', [64, 128, 256]),\n",
    "          'test_mode' : True,\n",
    "          'weighted' : hp.choice('weighted', [None, 'regular', 'double']),\n",
    "          'eval_batch_size' : 256,\n",
    "          'optimizer': hp.choice('optimizer', [\n",
    "              {'type': 'adam', \n",
    "               'lr' : hp.loguniform('lr_mult', -1, 3)}, \n",
    "              {'type' : 'SGD', \n",
    "               'lr' :  hp.loguniform('lr_mult', -1, 3)} ]),\n",
    "          'loss': hp.choice('loss', [\n",
    "              {'type': 'binary_crossentropy'}, \n",
    "              {'type' : 'focal_loss', \n",
    "               'alpha' :  hp.uniform('alpha', -1, 3), \n",
    "               'gamma' : hp.choice('gamma', [0.5, 0.75, 1.0])} ]),\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size' : hp.choice('batch_size', [8, 16, 32]), \n",
    "          'weighted' : hp.choice('weighted', [True, False]), \n",
    "          'layer_1' : hp.choice('layer_1', [{'kernel_1': hp.choice('kernel_1', [7, 11, 15, 19, 23, 27]), \n",
    "                                             'filters_1' : hp.choice('filters_1', [16, 32, 64, 128])}]), \n",
    "          'layer_2' : hp.choice('layer_2', [{'kernel_2': hp.choice('kernel_2', [7, 11, 15, 19, 23, 27]), \n",
    "                                             'filters_2' : hp.choice('filters_2', [16, 32, 64, 128])}])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'testing_with_hyper_opt_final_tf' does not exist. Creating a new experiment\n",
      "Experiment Name:  testing_with_hyper_opt_final_tf\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = 'testing_with_hyper_opt_final_tf'\n",
    "#mlflow.set_tracking_uri(URI) # If you have an instance\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "print(\"Experiment Name: \", EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model params\n",
    "def build_model(time_split, params):\n",
    "    inputs = tf.keras.Input(shape=(time_split, 3), name='feature')\n",
    "    x = tf.keras.layers.BatchNormalization(axis=2)(inputs)\n",
    "    x_1 = tf.keras.layers.Conv1D(filters=params['layer_1']['filters_1'], \n",
    "                                 kernel_size=params['layer_1']['kernel_1'])(x)\n",
    "    x_1 = tf.keras.layers.GlobalMaxPool1D()(x_1)\n",
    "    x_3 = tf.keras.layers.Conv1D(filters=params['layer_2']['filters_2'], \n",
    "                                 kernel_size=params['layer_2']['kernel_2'])(x)\n",
    "    x_3 = tf.keras.layers.GlobalMaxPool1D()(x_3)\n",
    "    x_5 = tf.keras.layers.Conv1D(filters=32, kernel_size=63)(x)\n",
    "    x_5 = tf.keras.layers.GlobalMaxPool1D()(x_5)\n",
    "    x = tf.keras.layers.Concatenate()([x_1, x_3, x_5])\n",
    "    x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid', name='label')(x)\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=output)\n",
    "    return model\n",
    "\n",
    "def build_optimizer():\n",
    "    optimizer='adam'\n",
    "    return optimizer\n",
    "\n",
    "def build_loss():\n",
    "    loss='binary_crossentropy'\n",
    "    return loss\n",
    "\n",
    "def build_metrics():\n",
    "    metrics=['accuracy']\n",
    "    return metrics\n",
    "\n",
    "def compile_model(model, optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']):\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_ds=None, valid_ds=None):\n",
    "    model.fit()\n",
    "\n",
    "def save_summary(line, file='model_summary.txt'):\n",
    "    with open(file, 'a') as f:\n",
    "        f.write(line +'\\n')\n",
    "    \n",
    "def build_class_weights(weight_type):\n",
    "    if weight_type:\n",
    "        class_weight = {1: 0.8, 0: 0.2}\n",
    "    else:\n",
    "        class_weight = None\n",
    "    return class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_callbacks(params):\n",
    "    callbacks = []\n",
    "    if 'es' in params['train']['params']['callbacks']:\n",
    "        es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                               patience=5,\n",
    "                                               mode='max',\n",
    "                                               restore_best_weights=True)\n",
    "        callbacks.append(es_callback)\n",
    "    elif 'cp' in params['train']['params']['callbacks']:\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         verbose=1)\n",
    "        callbacks.append(cp_callback)\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                               patience=5,\n",
    "                                               mode='max',\n",
    "                                               restore_best_weights=True)\n",
    "callbacks = [es_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_key_value_pair(kv_pairs, key, dictionary):\n",
    "    if type(dictionary) is dict:\n",
    "        for new_key, new_value in dictionary.items():\n",
    "            if key is not None:\n",
    "                transform_key_value_pair(kv_pairs, key + '_' + str(new_key), new_value)\n",
    "            else:\n",
    "                transform_key_value_pair(kv_pairs, new_key, new_value)    \n",
    "    else:\n",
    "        kv_pairs[key] = dictionary\n",
    "        \n",
    "def example(**args):\n",
    "    print(args)\n",
    "    for arg in args:\n",
    "        print(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objetive(params):\n",
    "    print('params: ',params)\n",
    "    summary_filename = 'summary.txt'\n",
    "    if os.path.exists(summary_filename):\n",
    "        os.remove(summary_filename)\n",
    "    run_name = 'model_' + str(uuid.uuid4())[:5]\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        out_run = run.to_dictionary()\n",
    "        print(out_run['info']['run_id'])\n",
    "        mlflow.tensorflow.autolog()\n",
    "        new_params = dict()\n",
    "        transform_key_value_pair(new_params, None, params)\n",
    "        #Model\n",
    "        model = build_model(time_split=time_split, params=params)\n",
    "        optimizer = build_optimizer()\n",
    "        loss = build_loss()\n",
    "        metrics = build_metrics()\n",
    "        model = compile_model(model=model, optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "        # model.summary(line_length=150, positions=[0.33, .55, .67, 1.],print_fn = lambda x: save_summary(x, file=summary_filename))\n",
    "        \n",
    "        #Build dataset\n",
    "        ds_train = ds.take(train_size)\n",
    "        ds_train = ds_train.repeat(count=epochs)\n",
    "        ds_train = ds_train.batch(params['batch_size'])\n",
    "        ds_train = ds_train.prefetch(1)\n",
    "\n",
    "        ds_valid = ds.skip(train_size).take(valid_size)\n",
    "        ds_valid = ds_valid.repeat(count=1)\n",
    "        ds_valid = ds_valid.batch(params['batch_size'])\n",
    "        ds_valid = ds_valid.prefetch(1)\n",
    "\n",
    "        ds_test = ds.skip(train_size+valid_size)\n",
    "        ds_test = ds_test.batch(eval_batch_size)\n",
    "        \n",
    "        #Train\n",
    "        \n",
    "        history = model.fit(ds_train, \n",
    "                    validation_data=ds_valid,\n",
    "                    steps_per_epoch=train_size//params['batch_size'],  \n",
    "                    epochs=epochs, \n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1, \n",
    "                    class_weight=build_class_weights(params['weighted']),\n",
    "                    shuffle=False)\n",
    "        # Metrics\n",
    "        mlflow.log_metric('val_accuracy', max(history.history['val_accuracy']))\n",
    "        for k, v in new_params.items():\n",
    "            mlflow.log_param(k, v)\n",
    "        # Artifacts\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        mlflow.end_run()\n",
    "    return {'loss': -max(history.history['val_accuracy']), 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:                                              \n",
      "{'batch_size': 8, 'layer_1': {'filters_1': 128, 'kernel_1': 11}, 'layer_2': {'filters_2': 16, 'kernel_2': 15}, 'weighted': False}\n",
      "c7a0d59839114c60883f3b4fab0381c8                     \n",
      "Train for 312 steps                                  \n",
      "Epoch 1/5                                            \n",
      "  1/312 [..............................]             \n",
      " - ETA: 4:54 - loss: 0.9015 - accuracy: 0.0000e+00   \n",
      "                                                    \n",
      "  8/312 [..............................]             \n",
      " - ETA: 38s - loss: 0.7140 - accuracy: 0.4219        \n",
      "                                                     \n",
      " 16/312 [>.............................]             \n",
      " - ETA: 20s - loss: 0.6823 - accuracy: 0.5000        \n",
      "                                                     \n",
      " 23/312 [=>............................]             \n",
      " - ETA: 14s - loss: 0.6455 - accuracy: 0.6413        \n",
      "                                                     \n",
      " 31/312 [=>............................]             \n",
      " - ETA: 11s - loss: 0.6268 - accuracy: 0.6976        \n",
      "                                                     \n",
      " 38/312 [==>...........................]             \n",
      " - ETA: 9s - loss: 0.6008 - accuracy: 0.7533         \n",
      "                                                    \n",
      " 46/312 [===>..........................]             \n",
      " - ETA: 7s - loss: 0.5927 - accuracy: 0.7690         \n",
      "                                                    \n",
      " 54/312 [====>.........................]             \n",
      " - ETA: 6s - loss: 0.6106 - accuracy: 0.7338         \n",
      "                                                    \n",
      " 61/312 [====>.........................]             \n",
      " - ETA: 6s - loss: 0.6163 - accuracy: 0.7275         \n",
      "                                                    \n",
      " 68/312 [=====>........................]             \n",
      " - ETA: 5s - loss: 0.6251 - accuracy: 0.6949         \n",
      "                                                    \n",
      " 74/312 [======>.......................]             \n",
      " - ETA: 5s - loss: 0.6162 - accuracy: 0.6976         \n",
      "                                                    \n",
      " 80/312 [======>.......................]             \n",
      " - ETA: 4s - loss: 0.6045 - accuracy: 0.7188         \n",
      "                                                    \n",
      " 87/312 [=======>......................]             \n",
      " - ETA: 4s - loss: 0.5955 - accuracy: 0.7356         \n",
      "                                                    \n",
      " 94/312 [========>.....................]             \n",
      " - ETA: 3s - loss: 0.6021 - accuracy: 0.7181         \n",
      "                                                    \n",
      "102/312 [========>.....................]             \n",
      " - ETA: 3s - loss: 0.5839 - accuracy: 0.7390         \n",
      "                                                    \n",
      "109/312 [=========>....................]             \n",
      " - ETA: 3s - loss: 0.5813 - accuracy: 0.7408         \n",
      "                                                    \n",
      "117/312 [==========>...................]             \n",
      " - ETA: 3s - loss: 0.5769 - accuracy: 0.7479         \n",
      "                                                    \n",
      "124/312 [==========>...................]             \n",
      " - ETA: 2s - loss: 0.5900 - accuracy: 0.7339         \n",
      "                                                    \n",
      "129/312 [===========>..................]             \n",
      " - ETA: 2s - loss: 0.5844 - accuracy: 0.7422         \n",
      "                                                    \n",
      "136/312 [============>.................]             \n",
      " - ETA: 2s - loss: 0.5785 - accuracy: 0.7472         \n",
      "                                                    \n",
      "143/312 [============>.................]             \n",
      " - ETA: 2s - loss: 0.5838 - accuracy: 0.7360         \n",
      "                                                    \n",
      "151/312 [=============>................]             \n",
      " - ETA: 2s - loss: 0.5774 - accuracy: 0.7425         \n",
      "                                                    \n",
      "158/312 [==============>...............]             \n",
      " - ETA: 2s - loss: 0.5702 - accuracy: 0.7516         \n",
      "                                                    \n",
      "166/312 [==============>...............]             \n",
      " - ETA: 1s - loss: 0.5626 - accuracy: 0.7560         \n",
      "                                                    \n",
      "173/312 [===============>..............]             \n",
      " - ETA: 1s - loss: 0.5642 - accuracy: 0.7536         \n",
      "                                                    \n",
      "181/312 [================>.............]             \n",
      " - ETA: 1s - loss: 0.5582 - accuracy: 0.7624         \n",
      "                                                    \n",
      "189/312 [=================>............]             \n",
      " - ETA: 1s - loss: 0.5588 - accuracy: 0.7566         \n",
      "                                                    \n",
      "197/312 [=================>............]             \n",
      " - ETA: 1s - loss: 0.5628 - accuracy: 0.7494         \n",
      "                                                    \n",
      "205/312 [==================>...........]             \n",
      " - ETA: 1s - loss: 0.5619 - accuracy: 0.7494         \n",
      "                                                    \n",
      "213/312 [===================>..........]             \n",
      " - ETA: 1s - loss: 0.5538 - accuracy: 0.7523         \n",
      "                                                    \n",
      "221/312 [====================>.........]             \n",
      " - ETA: 1s - loss: 0.5425 - accuracy: 0.7607         \n",
      "                                                    \n",
      "229/312 [=====================>........]             \n",
      " - ETA: 0s - loss: 0.5341 - accuracy: 0.7691         \n",
      "                                                    \n",
      "234/312 [=====================>........]             \n",
      " - ETA: 0s - loss: 0.5304 - accuracy: 0.7698         \n",
      "                                                    \n",
      "241/312 [======================>.......]             \n",
      " - ETA: 0s - loss: 0.5212 - accuracy: 0.7744         \n",
      "                                                    \n",
      "248/312 [======================>.......]             \n",
      " - ETA: 0s - loss: 0.5289 - accuracy: 0.7686         \n",
      "                                                    \n",
      "256/312 [=======================>......]             \n",
      " - ETA: 0s - loss: 0.5244 - accuracy: 0.7710         \n",
      "                                                    \n",
      "264/312 [========================>.....]             \n",
      " - ETA: 0s - loss: 0.5267 - accuracy: 0.7652         \n",
      "                                                    \n",
      "272/312 [=========================>....]             \n",
      " - ETA: 0s - loss: 0.5195 - accuracy: 0.7698         \n",
      "                                                    \n",
      "280/312 [=========================>....]             \n",
      " - ETA: 0s - loss: 0.5138 - accuracy: 0.7737         \n",
      "                                                    \n",
      "289/312 [==========================>...]             \n",
      " - ETA: 0s - loss: 0.5122 - accuracy: 0.7742         \n",
      "                                                    \n",
      "297/312 [===========================>..]             \n",
      " - ETA: 0s - loss: 0.5014 - accuracy: 0.7803         \n",
      "                                                    \n",
      "302/312 [============================>.]             \n",
      " - ETA: 0s - loss: 0.4968 - accuracy: 0.7835         \n",
      "                                                    \n",
      "309/312 [============================>.]             \n",
      " - ETA: 0s - loss: 0.4996 - accuracy: 0.7816         \n",
      "                                                    \n",
      "312/312 [==============================]             \n",
      " - 4s 14ms/step - loss: 0.5003 - accuracy: 0.7812 - val_loss: 0.7375 - val_accuracy: 0.6682\n",
      "\n",
      "Epoch 2/5                                            \n",
      " 1/28 [>.............................]               \n",
      " - ETA: 0s - loss: 0.2763 - accuracy: 1.0000         \n",
      "                                                    \n",
      " 9/28 [========>.....................]               \n",
      " - ETA: 0s - loss: 0.1282 - accuracy: 0.9583         \n",
      "                                                    \n",
      "16/28 [================>.............]               \n",
      " - ETA: 0s - loss: 0.1736 - accuracy: 0.9609         \n",
      "                                                    \n",
      "24/28 [========================>.....]               \n",
      " - ETA: 0s - loss: 0.1290 - accuracy: 0.9740         \n",
      "                                                    \n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s 9ms/step - loss: 0.1709 - accuracy: 0.9459 - val_loss: 0.5158 - val_accuracy: 0.7409\n",
      "\n",
      "Epoch 3/5                                            \n",
      " 1/28 [>.............................]               \n",
      " - ETA: 0s - loss: 0.0993 - accuracy: 1.0000         \n",
      "                                                    \n",
      " 9/28 [========>.....................]               \n",
      " - ETA: 0s - loss: 0.0200 - accuracy: 1.0000         \n",
      "                                                    \n",
      "17/28 [=================>............]               \n",
      " - ETA: 0s - loss: 0.0175 - accuracy: 1.0000         \n",
      "                                                    \n",
      "25/28 [=========================>....]               \n",
      " - ETA: 0s - loss: 0.0190 - accuracy: 1.0000         \n",
      "                                                    \n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================]\n",
      " - 3s 8ms/step - loss: 0.0661 - accuracy: 0.9880 - val_loss: 0.2604 - val_accuracy: 0.8636\n",
      "\n",
      "Epoch 4/5                                            \n",
      " 1/28 [>.............................]               \n",
      " - ETA: 0s - loss: 0.2292 - accuracy: 0.8750         \n",
      "                                                    \n",
      "10/28 [=========>....................]               \n",
      " - ETA: 0s - loss: 0.0317 - accuracy: 0.9875         \n",
      "                                                    \n",
      "17/28 [=================>............]               \n",
      " - ETA: 0s - loss: 0.0235 - accuracy: 0.9926         \n",
      "                                                    \n",
      "25/28 [=========================>....]               \n",
      " - ETA: 0s - loss: 0.0176 - accuracy: 0.9950         \n",
      "                                                    \n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================]\n",
      " - 3s 8ms/step - loss: 0.0159 - accuracy: 0.9960 - val_loss: 0.2569 - val_accuracy: 0.8909\n",
      "\n",
      "Epoch 5/5                                            \n",
      " 1/28 [>.............................]               \n",
      " - ETA: 0s - loss: 0.1064 - accuracy: 1.0000         \n",
      "                                                    \n",
      "10/28 [=========>....................]               \n",
      " - ETA: 0s - loss: 0.0131 - accuracy: 1.0000         \n",
      "                                                    \n",
      "18/28 [==================>...........]               \n",
      " - ETA: 0s - loss: 0.0079 - accuracy: 1.0000         \n",
      "                                                    \n",
      "26/28 [==========================>...]               \n",
      " - ETA: 0s - loss: 0.0057 - accuracy: 1.0000         \n",
      "                                                    \n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================]\n",
      " - 3s 8ms/step - loss: 0.0045 - accuracy: 0.9984 - val_loss: 0.2177 - val_accuracy: 0.9136\n",
      "\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.20s/trial, best loss: -0.9136363863945007]\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "_ = fmin(objetive, params, trials=trials, algo=tpe.suggest, max_evals=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def loguniform(low=0, high=1, size=None):\n",
    "    return np.exp(np.random.uniform(low, high, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = loguniform(low=-10.4, high=-2.8, size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(x, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.00003051, Max: 0.06069313998961354\n"
     ]
    }
   ],
   "source": [
    "print(\"Min: {:.8f}, Max: {}\".format(x.min(), x.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.lognormal(-6, 1, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004330304497495597"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(x, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00012093505262095613"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08559086951461047"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(uniform(low, high) / q) * q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quniform(low, high, q):\n",
    "    out = np.round(np.random.uniform(low, high, 300)/q)*q\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = quniform(0, 2, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5 , 1.75, 1.25, 1.25, 1.  , 1.25, 1.75, 0.5 , 0.  , 0.75, 2.  ,\n",
       "       0.  , 0.  , 1.  , 0.25, 0.75, 1.5 , 0.  , 2.  , 1.  , 1.  , 1.25,\n",
       "       1.75, 0.5 , 0.5 , 1.75, 1.25, 1.  , 1.  , 1.25, 0.25, 0.75, 1.  ,\n",
       "       0.75, 1.75, 0.75, 0.75, 1.5 , 0.25, 0.25, 1.75, 1.  , 0.25, 0.25,\n",
       "       0.75, 0.75, 0.25, 0.5 , 1.75, 2.  , 0.  , 0.  , 0.75, 0.  , 1.75,\n",
       "       0.5 , 1.5 , 0.75, 1.75, 1.75, 0.5 , 1.5 , 1.5 , 0.5 , 0.75, 1.5 ,\n",
       "       0.25, 2.  , 0.25, 1.25, 0.5 , 1.  , 1.5 , 1.75, 0.5 , 1.75, 1.75,\n",
       "       1.25, 0.75, 2.  , 2.  , 0.5 , 0.75, 1.  , 0.75, 2.  , 0.  , 0.25,\n",
       "       0.5 , 0.25, 0.5 , 0.25, 0.25, 1.  , 1.  , 1.75, 0.  , 1.  , 0.75,\n",
       "       0.75, 0.25, 0.5 , 1.5 , 0.75, 1.5 , 0.25, 2.  , 1.25, 0.  , 1.  ,\n",
       "       0.75, 1.25, 0.75, 0.75, 1.75, 1.25, 1.  , 0.  , 2.  , 0.5 , 1.75,\n",
       "       1.5 , 0.25, 0.  , 1.75, 1.25, 0.  , 0.25, 0.5 , 1.75, 0.75, 1.25,\n",
       "       1.25, 0.25, 0.25, 0.5 , 0.75, 0.5 , 1.75, 0.5 , 0.75, 0.25, 1.75,\n",
       "       0.25, 1.75, 0.5 , 2.  , 0.25, 1.5 , 1.25, 1.  , 1.75, 1.5 , 0.25,\n",
       "       1.75, 1.5 , 1.25, 1.  , 1.25, 1.5 , 1.5 , 1.5 , 0.5 , 1.  , 0.25,\n",
       "       1.  , 0.5 , 0.5 , 0.  , 1.75, 1.75, 1.25, 1.  , 0.5 , 0.25, 1.75,\n",
       "       1.  , 1.  , 0.5 , 1.5 , 1.5 , 1.75, 0.25, 1.5 , 1.75, 1.25, 0.5 ,\n",
       "       2.  , 0.25, 1.  , 2.  , 0.5 , 0.5 , 0.25, 1.25, 0.5 , 0.  , 0.  ,\n",
       "       1.25, 1.5 , 0.5 , 2.  , 1.5 , 1.75, 1.25, 0.25, 1.75, 1.25, 0.75,\n",
       "       0.5 , 1.75, 0.25, 0.5 , 0.75, 2.  , 1.25, 0.  , 0.  , 1.25, 1.5 ,\n",
       "       1.  , 1.  , 0.25, 1.5 , 0.5 , 2.  , 1.25, 0.  , 0.25, 0.  , 2.  ,\n",
       "       2.  , 1.25, 1.25, 1.5 , 2.  , 1.5 , 0.75, 1.25, 0.75, 0.5 , 1.  ,\n",
       "       0.5 , 0.25, 1.5 , 2.  , 0.25, 0.25, 2.  , 1.5 , 2.  , 1.75, 1.5 ,\n",
       "       1.5 , 0.  , 0.5 , 1.  , 0.25, 1.  , 0.5 , 1.25, 1.75, 0.25, 1.5 ,\n",
       "       0.25, 1.75, 0.5 , 0.25, 1.25, 0.  , 0.5 , 1.75, 1.5 , 0.25, 1.5 ,\n",
       "       0.25, 0.25, 0.75, 1.25, 0.25, 0.75, 0.5 , 1.75, 0.75, 0.5 , 1.5 ,\n",
       "       0.  , 1.  , 0.75, 0.75, 1.25, 0.75, 1.  , 1.75, 1.75, 2.  , 1.25,\n",
       "       0.25, 1.5 , 0.5 ])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('file.txt','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='file.txt' mode='w' encoding='UTF-8'>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.write(\"\" + \"/n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlflow_tune]",
   "language": "python",
   "name": "conda-env-mlflow_tune-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
