{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from ray import tune\n",
    "import os\n",
    "\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "DATA_PATH = '/Users/camilovelasquez/Desktop/Documents/Datasets/WISDM-Smartphones/wisdm-dataset/raw'\n",
    "ids = np.arange(1600, 1650)\n",
    "devices = ['phone']\n",
    "sensors = ['accel']\n",
    "activities = ['A', 'B']\n",
    "time_taken = 3000\n",
    "time_split = 100\n",
    "\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "EVAL_BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "\n",
    "train_size = 2500\n",
    "valid_size = 220\n",
    "test_size = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_WISDM_data(DATA_PATH, ids=np.arange(1600, 1650), \n",
    "                    devices=['phone'], sensors=['accel']):\n",
    "    \"\"\"Read from DATA PATH and create a pandas table from it\"\"\"\n",
    "    i = 0\n",
    "    for current_id in ids:\n",
    "        for current_device in devices:\n",
    "            for current_sensor in sensors:\n",
    "                file_path = os.path.join(DATA_PATH, current_device, current_sensor, \n",
    "                                         'data_{}_{}_{}.txt'.format(current_id, current_sensor, current_device))\n",
    "                if i==0:\n",
    "                    table = pd.read_csv(file_path, delimiter=',', \n",
    "                                        names=['ID', 'Activity Label', 'Timestamp', 'x', 'y', 'z'], \n",
    "                                        lineterminator='\\n')\n",
    "                else:\n",
    "                    aux = pd.read_csv(file_path, delimiter=',', \n",
    "                                      names=['ID', 'Activity Label', 'Timestamp', 'x', 'y', 'z'], \n",
    "                                        lineterminator='\\n')\n",
    "                    table = pd.concat([table, aux], axis=0)\n",
    "                i+=1\n",
    "    table.loc[:,'z'] = table.z.str.replace(';','').astype(np.float32)\n",
    "    return table\n",
    "\n",
    "def transform_data(table, time_taken, time_split):\n",
    "    \"\"\"Transform data from raw table into a zip of (features, labels),\n",
    "        where features has shape (samples, time_steps, features), and labels (samples,)\"\"\"\n",
    "    table = table.set_index(['ID', 'Activity Label'])\n",
    "    table = table.groupby(['ID', 'Activity Label']).head(time_taken)\n",
    "    timestamp_edit = np.tile(np.arange(0,time_split), int(table.shape[0]/time_split))\n",
    "    table['Timestamp'] = timestamp_edit\n",
    "    table = table.reset_index().set_index(['ID', 'Activity Label', 'Timestamp'], append=True)\n",
    "    features = table.values.reshape((int(table.shape[0]/time_split), time_split, table.shape[1]))\n",
    "    labels = table.reset_index()['Activity Label']\\\n",
    "        .values[np.arange(0,int(table.shape[0]/time_split)*time_split, time_split)]\n",
    "    return features, labels\n",
    "\n",
    "def preprocessing_data(table, time_taken=3000, time_split=100, activities=['A', 'B']):\n",
    "    \"\"\"Preprocess table and convert it into tf dataset\"\"\"\n",
    "    features, labels = transform_data(table, time_taken=time_taken, time_split=time_split)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({'feature': features}, {'label': labels}))\n",
    "    ds = dataset.filter(lambda x, y: tf.reduce_any(tf.equal(y['label'], activities))==True)\n",
    "    ds = ds.map(label2prob)\n",
    "    return ds\n",
    "\n",
    "def label2prob(feature, label):\n",
    "    new_label = tf.where(tf.equal(label['label'], 'A'), 1, 0)\n",
    "    label['label'] = new_label\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = read_WISDM_data(DATA_PATH, ids=ids, devices=devices, sensors=sensors)\n",
    "ds = preprocessing_data(table, time_taken=time_taken, time_split=time_split, activities=activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=12259, shape=(), dtype=int32, numpy=2940>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.reduce(0, lambda x,_: x+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(time_split):\n",
    "    inputs = tf.keras.Input(shape=(time_split, 3), name='feature')\n",
    "    x = tf.keras.layers.BatchNormalization(axis=2)(inputs)\n",
    "    x_1 = tf.keras.layers.Conv1D(filters=32, kernel_size=15)(x)\n",
    "    x_1 = tf.keras.layers.GlobalMaxPool1D()(x_1)\n",
    "    x_3 = tf.keras.layers.Conv1D(filters=32, kernel_size=31)(x)\n",
    "    x_3 = tf.keras.layers.GlobalMaxPool1D()(x_3)\n",
    "    x_5 = tf.keras.layers.Conv1D(filters=32, kernel_size=63)(x)\n",
    "    x_5 = tf.keras.layers.GlobalMaxPool1D()(x_5)\n",
    "    x = tf.keras.layers.Concatenate()([x_1, x_3, x_5])\n",
    "    x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid', name='label')(x)\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=output)\n",
    "    return model\n",
    "\n",
    "def build_optimizer():\n",
    "    optimizer='adam'\n",
    "    return optimizer\n",
    "\n",
    "def build_loss():\n",
    "    loss='binary_crossentropy'\n",
    "    return loss\n",
    "\n",
    "def build_metrics():\n",
    "    metrics=['accuracy']\n",
    "    return metrics\n",
    "\n",
    "def compile_model(model, optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']):\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_ds=None, valid_ds=None):\n",
    "    model.fit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(time_split=time_split)\n",
    "optimizer = build_optimizer()\n",
    "loss = build_loss()\n",
    "metrics = build_metrics()\n",
    "model = compile_model(model=model, optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "feature (InputLayer)            [(None, 100, 3)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 100, 3)       12          feature[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 86, 32)       1472        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 70, 32)       3008        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 38, 32)       6080        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 32)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 32)           0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 32)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 96)           0           global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1552        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "label (Dense)                   (None, 1)            17          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,141\n",
      "Trainable params: 12,135\n",
      "Non-trainable params: 6\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds.take(train_size)\n",
    "ds_train = ds_train.shuffle(buffer_size=TRAIN_BATCH_SIZE)\n",
    "ds_train = ds_train.repeat(count=EPOCHS)\n",
    "ds_train = ds_train.batch(TRAIN_BATCH_SIZE)\n",
    "ds_train = ds_train.prefetch(1)\n",
    "\n",
    "ds_valid = ds.skip(train_size).take(valid_size)\n",
    "ds_valid = ds_valid.repeat(count=1)\n",
    "ds_valid = ds_valid.batch(TRAIN_BATCH_SIZE)\n",
    "ds_valid = ds_valid.prefetch(1)\n",
    "\n",
    "ds_test = ds.skip(train_size+valid_size)\n",
    "ds_test = ds_test.batch(EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                               patience=EPOCHS//2,\n",
    "                                               mode='max',\n",
    "                                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size//TRAIN_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 312 steps\n",
      "Epoch 1/10\n",
      "312/312 [==============================] - 4s 12ms/step - loss: 0.4654 - accuracy: 0.7692 - val_loss: 0.3391 - val_accuracy: 0.7864\n",
      "Epoch 2/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 3s 8ms/step - loss: 0.1174 - accuracy: 0.9391 - val_loss: 0.1766 - val_accuracy: 0.9136\n",
      "Epoch 3/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 2s 8ms/step - loss: 0.0718 - accuracy: 0.9884 - val_loss: 0.0797 - val_accuracy: 0.9864\n",
      "Epoch 4/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 3s 8ms/step - loss: 0.0104 - accuracy: 0.9980 - val_loss: 0.1019 - val_accuracy: 0.9773\n",
      "Epoch 5/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 2s 8ms/step - loss: 0.0055 - accuracy: 0.9988 - val_loss: 0.0730 - val_accuracy: 0.9864\n",
      "Epoch 6/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 3s 8ms/step - loss: 0.1311 - accuracy: 0.9988 - val_loss: 0.0675 - val_accuracy: 0.9864\n",
      "Epoch 7/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 2s 8ms/step - loss: 0.0052 - accuracy: 0.9992 - val_loss: 0.0635 - val_accuracy: 0.9864\n",
      "Epoch 8/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 2s 8ms/step - loss: 0.0074 - accuracy: 0.9992 - val_loss: 0.0652 - val_accuracy: 0.9864\n",
      "Epoch 9/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 2s 8ms/step - loss: 0.0127 - accuracy: 0.9996 - val_loss: 0.0608 - val_accuracy: 0.9909\n",
      "Epoch 10/10\n",
      "312/28 [==============================================================================================================================================================================================================================================================================================================================================] - 2s 8ms/step - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 0.9909\n"
     ]
    }
   ],
   "source": [
    "callbacks = [es_callback]\n",
    "history = model.fit(ds_train, \n",
    "                    validation_data=ds_valid,\n",
    "                    steps_per_epoch=train_size//TRAIN_BATCH_SIZE,  \n",
    "                    epochs=EPOCHS, \n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1, \n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 71ms/step - loss: 0.1214 - accuracy: 0.9636\n"
     ]
    }
   ],
   "source": [
    "test_result = model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlflow_tune]",
   "language": "python",
   "name": "conda-env-mlflow_tune-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
